The rapid advancement and deployment of Large Language Models (LLMs) necessitate strict regulations to ensure user safety, ethical usage, and societal wellbeing. 

Firstly, LLMs possess the capability to generate convincing yet misleading information, which can spread misinformation and cause societal harm. Without stringent laws, malicious actors could exploit these tools, creating deepfakes or propagating harmful narratives, leading to chaos in public discourse.

Secondly, the potential for LLMs to perpetuate biases is alarming. These models learn from vast datasets that may contain latent biases, and if left unregulated, they could reinforce stereotypes and discrimination, exacerbating social inequalities. Implementing strict laws would ensure that developers are held accountable for mitigating bias and ensuring fairness in AI outputs.

Moreover, LLMs raise significant privacy concerns as they can unintentionally generate sensitive personal information by parsing data that was included in their training sets. Strict regulations would enforce data protection laws, ensuring that user privacy is preserved and respected.

Lastly, regulatory frameworks would foster transparency and establish standards for model training and deployment. This would promote ethical AI practices, ensuring that LLMs are not only powerful tools but also developed and utilized responsibly.

In conclusion, strict laws to regulate LLMs are essential to safeguard individuals and society at large, protect against harm and biases, respect privacy, and promote ethical standards in AI technology. The need for regulation is clear; to ensure that the evolution of AI benefits humanity rather than posing risks.